#!/usr/bin/env python3
"""
Add follow-up tasks for Task 8: End-to-End Testing
"""

import json
from pathlib import Path
from datetime import datetime

def add_task_to_json(title, description, priority, details, test_strategy):
    """Add a task directly to the tasks.json file"""
    tasks_file = Path(".taskmaster/tasks/tasks.json")
    
    # Load existing tasks
    with open(tasks_file, 'r') as f:
        data = json.load(f)
    
    # Find next ID - handle both int and string IDs
    int_ids = []
    for t in data['tasks']:
        if 'id' in t:
            try:
                int_ids.append(int(t['id']))
            except (ValueError, TypeError):
                # Skip non-integer IDs
                pass
    next_id = max(int_ids, default=0) + 1
    
    # Create new task
    new_task = {
        'id': next_id,
        'title': title,
        'description': description,
        'status': 'pending',
        'dependencies': [],
        'priority': priority,
        'details': details,
        'testStrategy': test_strategy,
        'createdAt': datetime.now().isoformat(),
        'updatedAt': datetime.now().isoformat(),
        'subtasks': []
    }
    
    # Add to tasks
    data['tasks'].append(new_task)
    
    # Update meta if it exists
    if 'meta' in data:
        data['meta']['updatedAt'] = datetime.now().isoformat()
    
    # Save back
    with open(tasks_file, 'w') as f:
        json.dump(data, f, indent=2)
    
    return next_id

def main():
    # Add follow-up tasks for comprehensive testing improvements
    tasks_to_add = [
        {
            "title": "Implement detailed test metrics reporting",
            "description": "Create comprehensive test reports with: test count per category, code coverage percentage, performance benchmarks (response times, throughput), and detailed security scan results",
            "priority": "high",
            "details": "The current test results lack specific metrics. We need detailed reporting including:\n- Total test count and breakdown by type\n- Code coverage percentage and uncovered areas\n- Performance benchmarks with actual numbers\n- Security scan tool names and vulnerability categories checked",
            "test_strategy": "Validate that reports contain all required metrics and are generated in both human-readable and machine-parseable formats"
        },
        {
            "title": "Add end-to-end user journey tests",
            "description": "Create comprehensive user journey tests that simulate real user workflows through the entire feedback system",
            "priority": "high",
            "details": "Current tests appear to be isolated. We need end-to-end tests that:\n- Simulate complete user workflows from start to finish\n- Test multiple user roles and permissions\n- Validate data flow through all system components\n- Include edge cases and error scenarios",
            "test_strategy": "Use automated testing tools to simulate user interactions and validate expected outcomes at each step"
        },
        {
            "title": "Implement continuous integration test automation",
            "description": "Set up automated test execution in CI/CD pipeline with failure notifications and trend tracking",
            "priority": "medium",
            "details": "Ensure all tests run automatically on:\n- Every commit to main branch\n- All pull requests\n- Scheduled nightly runs for extensive tests\n- Include test result trending and failure analysis",
            "test_strategy": "Verify CI pipeline configuration and test execution logs"
        },
        {
            "title": "Create test data management system",
            "description": "Implement a system for managing test data including creation, cleanup, and isolation between test runs",
            "priority": "medium",
            "details": "Develop a test data management system that:\n- Generates realistic test data\n- Ensures test isolation\n- Handles data cleanup after tests\n- Supports different test environments",
            "test_strategy": "Validate data isolation between test runs and proper cleanup mechanisms"
        },
        {
            "title": "Add performance regression testing",
            "description": "Implement performance regression tests to detect performance degradation between releases",
            "priority": "medium",
            "details": "Create performance regression suite that:\n- Establishes baseline performance metrics\n- Compares current performance against baselines\n- Alerts on significant performance degradation\n- Tracks performance trends over time",
            "test_strategy": "Run performance tests against previous versions and validate detection of intentional performance degradation"
        }
    ]
    
    # Add each task
    created_ids = []
    for task_data in tasks_to_add:
        task_id = add_task_to_json(
            title=task_data["title"],
            description=task_data["description"],
            priority=task_data["priority"],
            details=task_data["details"],
            test_strategy=task_data["test_strategy"]
        )
        created_ids.append(task_id)
        print(f"âœ… Created task {task_id}: {task_data['title']}")
    
    print(f"\nðŸ“‹ Successfully created {len(created_ids)} follow-up tasks for Task 8")
    
    # Display summary
    print("\nFollow-up tasks created:")
    for i, task_data in enumerate(tasks_to_add):
        print(f"  - Task {created_ids[i]} [{task_data['priority']}]: {task_data['title']}")

if __name__ == "__main__":
    main()